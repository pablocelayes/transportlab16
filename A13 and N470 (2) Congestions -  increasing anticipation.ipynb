{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our previous explorations, we saw that as expected, the quality of predictions begins to drop down as we try to make the anticipation period longer. Nevertheless, for 20 and 30 minutes we still see some decent results and at the same time these periods are already good enough to serve a good informative purpose (for instance, allowing commuters to consider alternative routes to and back from work)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 minutes anticipation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will work on trying to improve as much as possible the quality of predictions with 30 minutes of anticipation for Q2, possibly by adding some additional datasets. Later we will see how much of this can be extrapolated to other quarters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import pickle   \n",
    "\n",
    "n470_hects = range(53,70)\n",
    "\n",
    "ngbr_hects = range(33,90)\n",
    "\n",
    "rf = pd.read_csv('a13/Rechts_Flow_2011.csv', header=None)\n",
    "rs = pd.read_csv('a13/Rechts_Speed_2011.csv', header=None)\n",
    "\n",
    "rs_intersection = rs.iloc[:,n470_hects]\n",
    "\n",
    "inters_means = rs_intersection.mean(axis=1)\n",
    "\n",
    "jam_threshold = 95\n",
    "\n",
    "mins_diff = 30\n",
    "y = (inters_means < jam_threshold)[mins_diff:]\n",
    "\n",
    "df = pd.concat([rf.iloc[:,ngbr_hects], rs.iloc[:,ngbr_hects]],axis=1)\n",
    "df = df.iloc[:-mins_diff,:]\n",
    "\n",
    "quarter_size = df.shape[0]//4\n",
    "\n",
    "df = df.iloc[quarter_size:2*quarter_size,:]\n",
    "y = y[quarter_size:2*quarter_size]\n",
    "\n",
    "\n",
    "def train_test_split(df, y, test_size=0.3):\n",
    "    cut = int(len(y) * test_size)\n",
    "\n",
    "    X_train = df.iloc[:-cut,:]\n",
    "    X_test = df.iloc[-cut:,:]\n",
    "\n",
    "    y_train = y[:-cut]\n",
    "    y_test = y[-cut:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(df, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use a the GridSearchCV modeule from sklearn to find the best combination of parameters for our problem, using 3-fold cross validation for each candidate combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=sklearn.cross_validation.StratifiedKFold(labels=[False False ..., False False], n_folds=3, shuffle=False, random_state=None),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'min_samples_split': [10, 20], 'max_depth': [5, 10, None], 'max_features': [50, 100], 'n_estimators': [10, 20], 'criterion': ['gini', 'entropy'], 'min_samples_leaf': [3]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"max_depth\": [5, 10, None],\n",
    "          \"max_features\": [50, 100],\n",
    "          \"min_samples_split\": [10, 20],\n",
    "          \"min_samples_leaf\": [3],\n",
    "          \"criterion\": [\"gini\", \"entropy\"],\n",
    "          \"n_estimators\": [10, 20]\n",
    "         }\n",
    "\n",
    "clf = GridSearchCV(\n",
    "    RandomForestClassifier(),  \n",
    "    param_grid=params,  # parameters to tune via cross validation\n",
    "    refit=True,  # fit using all data, on the best detected classifier\n",
    "    n_jobs=-1,  # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "    scoring='f1',  # what score are we optimizing?\n",
    "    cv=StratifiedKFold(y_train, n_folds=3),  # what type of cross validation to use\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the parameter for the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_depth': 5,\n",
       " 'max_features': 50,\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 10,\n",
       " 'n_estimators': 10}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.93      0.96      0.95     31370\n",
      "       True       0.82      0.73      0.77      8047\n",
      "\n",
      "avg / total       0.91      0.91      0.91     39417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a 3% improvement in congestion recall, while sacrificing only 1% in precision, relative to the model that we had before:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    precision    recall  f1-score   support\n",
    "\n",
    "      False       0.93      0.96      0.95     31372\n",
    "       True       0.83      0.70      0.76      8047\n",
    "\n",
    "avg / total       0.91      0.91      0.91     39419\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve this results further by adding other datasets and doing some feature engineering.\n",
    "But that will be the subject for future explorations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 minutes of anticipation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us see if we can do some improvement for 20 minutes of anticipation.\n",
    "\n",
    "These were the resuls for the previous model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    precision    recall  f1-score   support\n",
    "\n",
    "      False       0.94      0.97      0.96     31372\n",
    "       True       0.87      0.77      0.82      8047\n",
    "\n",
    "avg / total       0.93      0.93      0.93     39419\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mins_diff = 20\n",
    "y = (inters_means < jam_threshold)[mins_diff:]\n",
    "\n",
    "df = pd.concat([rf.iloc[:,ngbr_hects], rs.iloc[:,ngbr_hects]],axis=1)\n",
    "df = df.iloc[:-mins_diff,:]\n",
    "\n",
    "quarter_size = df.shape[0]//4\n",
    "\n",
    "df = df.iloc[quarter_size:2*quarter_size,:]\n",
    "y = y[quarter_size:2*quarter_size]\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=sklearn.cross_validation.StratifiedKFold(labels=[False False ..., False False], n_folds=3, shuffle=False, random_state=None),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'min_samples_split': [10, 20], 'max_depth': [5, 10, None], 'max_features': [50, 100], 'n_estimators': [10, 20], 'criterion': ['gini', 'entropy'], 'min_samples_leaf': [3]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(\n",
    "    RandomForestClassifier(),  \n",
    "    param_grid=params,  # parameters to tune via cross validation\n",
    "    refit=True,  # fit using all data, on the best detected classifier\n",
    "    n_jobs=-1,  # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "    scoring='f1',  # what score are we optimizing?\n",
    "    cv=StratifiedKFold(y_train, n_folds=3),  # what type of cross validation to use\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.95      0.97      0.96     31371\n",
      "       True       0.87      0.78      0.82      8047\n",
      "\n",
      "avg / total       0.93      0.93      0.93     39418\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we got a 1% improvement in recall for congestions without sacrificing any other metrics.\n",
    "Not a huge improvement here either."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
