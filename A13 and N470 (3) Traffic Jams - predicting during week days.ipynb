{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analysis made here by Sergio Rossell, we will try to train some models that predict traffic jams during weekdays.\n",
    "\n",
    "https://github.com/pablocelayes/transportlab16/blob/master/jamsOnDaysAndHours.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will experiment using 5, 15 or 30 minutes of anticipation. For now we will focus on data from Q2, and later we will see how this results extend to other quarters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(mins_diff):\n",
    "    # n470_hects = range(53,70) # Hectometers from previous analysis\n",
    "    n470_hects = range(47,68) # Hectometers between 10.087 and 12.110, as used in Sergio's analysis\n",
    "    ngbr_hects = range(n470_hects[0] - 15, n470_hects[-1] + 15)\n",
    "    jam_threshold = 30\n",
    "    times = pd.date_range(start = datetime(2011, 1, 1), end = datetime(2011, 12, 31, 23, 59), freq = 'T')\n",
    "    \n",
    "    rf = pd.read_csv('a13/Rechts_Flow_2011.csv', header=None)\n",
    "    rs = pd.read_csv('a13/Rechts_Speed_2011.csv', header=None)\n",
    "    df = pd.concat([rf.iloc[:,ngbr_hects], rs.iloc[:,ngbr_hects]],axis=1)\n",
    "    \n",
    "    rs.set_index(times, inplace = True)\n",
    "    df.set_index(times, inplace = True)\n",
    "    \n",
    "    # keeping only weekdays in Q2\n",
    "    rsi = rs.iloc[:,n470_hects]\n",
    "    rsi = rsi[(rsi.index.dayofweek < 5) & (rsi.index.month >= 3) & (rsi.index.month <= 6)]\n",
    "    inters_means = rsi.mean(axis=1)\n",
    "\n",
    "    df = df[(df.index.dayofweek < 5) & (df.index.month >= 3) & (df.index.month <= 6)]\n",
    "    df = df.iloc[:-mins_diff,:]\n",
    "    \n",
    "    y = (inters_means < jam_threshold)[mins_diff:]\n",
    "\n",
    "    return df, y.values\n",
    "\n",
    "\n",
    "def train_test_split(df, y, test_size=0.3):\n",
    "    cut = int(df.shape[0] * test_size)\n",
    "    \n",
    "    df['y'] = y\n",
    "    X_train = df.iloc[:-cut,:]\n",
    "    X_test = df.iloc[-cut:,:]\n",
    "\n",
    "    y_train = X_train['y'].values; del X_train['y']\n",
    "    y_test = X_test['y'].values; del X_test['y']\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the GridSearchCV module from sklearn to find the best combination of parameters for our problem, using 3-fold cross validation for each candidate combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching best model for 5 mins of anticipation\n",
      "{'criterion': 'entropy', 'max_depth': None, 'class_weight': 'balanced', 'min_samples_leaf': 3, 'max_features': 25, 'min_samples_split': 10, 'n_estimators': 20}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      0.99      0.99     36876\n",
      "       True       0.82      0.86      0.84      1138\n",
      "\n",
      "avg / total       0.99      0.99      0.99     38014\n",
      "\n",
      "Searching best model for 15 mins of anticipation\n",
      "{'criterion': 'gini', 'max_depth': None, 'class_weight': 'balanced_subsample', 'min_samples_leaf': 3, 'max_features': 25, 'min_samples_split': 20, 'n_estimators': 10}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.98      0.98      0.98     36873\n",
      "       True       0.37      0.38      0.37      1138\n",
      "\n",
      "avg / total       0.96      0.96      0.96     38011\n",
      "\n",
      "Searching best model for 30 mins of anticipation\n",
      "{'criterion': 'gini', 'max_depth': 10, 'class_weight': 'balanced_subsample', 'min_samples_leaf': 3, 'max_features': 25, 'min_samples_split': 10, 'n_estimators': 20}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.88      0.93     36869\n",
      "       True       0.17      0.83      0.29      1138\n",
      "\n",
      "avg / total       0.97      0.88      0.91     38007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\"max_depth\": [5, 10, None],\n",
    "          \"max_features\": [25, 50],\n",
    "          \"class_weight\": [\"balanced\", \"balanced_subsample\"],\n",
    "          \"min_samples_split\": [10, 20],\n",
    "          \"min_samples_leaf\": [3],\n",
    "          \"criterion\": [\"gini\", \"entropy\"],\n",
    "          \"n_estimators\": [10, 20],\n",
    "          \n",
    "         }\n",
    "\n",
    "for mins in [5, 15, 30]:\n",
    "    print(\"Searching best model for %d mins of anticipation\" % mins)\n",
    "    df, y = create_dataset(mins)\n",
    "    X_train, X_test, y_train, y_test =  train_test_split(df, y)\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        RandomForestClassifier(),  \n",
    "        param_grid=params,  # parameters to tune via cross validation\n",
    "        refit=True,  # fit using all data, on the best detected classifier\n",
    "        n_jobs=-1,  # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "        scoring='f1',  # what score are we optimizing?\n",
    "        cv=StratifiedKFold(y_train, n_folds=3),  # what type of cross validation to use\n",
    "    )\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(clf.best_params_)\n",
    "\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance is very good for 5 mins of anticipation, but then degrades a lot for 15 mins.\n",
    "Let's see how much we can extend the anticipation before it starts to degrade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching best model for 6 mins of anticipation\n",
      "{'criterion': 'entropy', 'max_depth': None, 'class_weight': 'balanced_subsample', 'min_samples_leaf': 3, 'max_features': 25, 'min_samples_split': 20, 'n_estimators': 10}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      0.99      0.99     36876\n",
      "       True       0.76      0.85      0.80      1138\n",
      "\n",
      "avg / total       0.99      0.99      0.99     38014\n",
      "\n",
      "Searching best model for 7 mins of anticipation\n",
      "{'criterion': 'entropy', 'max_depth': None, 'class_weight': 'balanced', 'min_samples_leaf': 3, 'max_features': 25, 'min_samples_split': 10, 'n_estimators': 20}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.99      0.99     36875\n",
      "       True       0.77      0.79      0.78      1138\n",
      "\n",
      "avg / total       0.99      0.99      0.99     38013\n",
      "\n",
      "Searching best model for 8 mins of anticipation\n",
      "{'criterion': 'entropy', 'max_depth': None, 'class_weight': 'balanced_subsample', 'min_samples_leaf': 3, 'max_features': 25, 'min_samples_split': 20, 'n_estimators': 20}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.99      0.99     36875\n",
      "       True       0.69      0.81      0.75      1138\n",
      "\n",
      "avg / total       0.98      0.98      0.98     38013\n",
      "\n",
      "Searching best model for 9 mins of anticipation\n",
      "{'criterion': 'entropy', 'max_depth': None, 'class_weight': 'balanced_subsample', 'min_samples_leaf': 3, 'max_features': 25, 'min_samples_split': 10, 'n_estimators': 20}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.99      0.99     36875\n",
      "       True       0.72      0.67      0.69      1138\n",
      "\n",
      "avg / total       0.98      0.98      0.98     38013\n",
      "\n",
      "Searching best model for 10 mins of anticipation\n",
      "{'criterion': 'entropy', 'max_depth': None, 'class_weight': 'balanced_subsample', 'min_samples_leaf': 3, 'max_features': 25, 'min_samples_split': 20, 'n_estimators': 20}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.99      0.99     36875\n",
      "       True       0.60      0.67      0.63      1138\n",
      "\n",
      "avg / total       0.98      0.98      0.98     38013\n",
      "\n",
      "Searching best model for 11 mins of anticipation\n",
      "{'criterion': 'gini', 'max_depth': None, 'class_weight': 'balanced_subsample', 'min_samples_leaf': 3, 'max_features': 50, 'min_samples_split': 20, 'n_estimators': 20}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.98      0.98     36874\n",
      "       True       0.50      0.61      0.55      1138\n",
      "\n",
      "avg / total       0.97      0.97      0.97     38012\n",
      "\n",
      "Searching best model for 12 mins of anticipation\n"
     ]
    }
   ],
   "source": [
    "params = {\"max_depth\": [5, 10, None],\n",
    "          \"max_features\": [25, 50],\n",
    "          \"class_weight\": [\"balanced\", \"balanced_subsample\"],\n",
    "          \"min_samples_split\": [10, 20],\n",
    "          \"min_samples_leaf\": [3],\n",
    "          \"criterion\": [\"gini\", \"entropy\"],\n",
    "          \"n_estimators\": [10, 20],\n",
    "          \n",
    "         }\n",
    "\n",
    "for mins in range(6,15):\n",
    "    print(\"Searching best model for %d mins of anticipation\" % mins)\n",
    "    df, y = create_dataset(mins)\n",
    "    X_train, X_test, y_train, y_test =  train_test_split(df, y)\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        RandomForestClassifier(),  \n",
    "        param_grid=params,  # parameters to tune via cross validation\n",
    "        refit=True,  # fit using all data, on the best detected classifier\n",
    "        n_jobs=-1,  # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "        scoring='f1',  # what score are we optimizing?\n",
    "        cv=StratifiedKFold(y_train, n_folds=3),  # what type of cross validation to use\n",
    "    )\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(clf.best_params_)\n",
    "\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
